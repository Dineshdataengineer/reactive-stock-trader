lagom.serialization.json.migrations {
  # Uses class loader which is broken in the Lagom dev environment (presumably in the same way Play is)
  # "com.redelastic.stocktrader.portfolio.impl.PortfolioEvent" = "com.redelastic.stocktrader.portfolio.impl.migrations.PortfolioEventMigration"
}

play {
  akka.actor-system = "portfolio-service"
  http.secret.key = "changeme"
  http.secret.key = ${?APPLICATION_SECRET}
  server.pidfile.path=/dev/null
}

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
  "akka.Done" = akka-misc
  "akka.actor.Address" = akka-misc
  "akka.remote.UniqueAddress" = akka-misc
}

akka.cluster.sharding.state-store-mode = ddata

######################################
# Persistence (Cassandra) Configuration
######################################

portfolio.cassandra.keyspace = portfolio

cassandra-journal {
  keyspace = ${portfolio.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}

cassandra-snapshot-store {
  keyspace = ${portfolio.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}

lagom.persistence.read-side.cassandra {
  keyspace = ${portfolio.cassandra.keyspace}
  keyspace-autocreate = true
}

cassandra-query-journal {
  eventual-consistency-delay = 200ms
  delayed-event-timeout = 30s
  refresh-interval = 1s
}

lagom.services {
  broker = ${?BROKER_SERVICE_URL}
}

lagom.persistence {
  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 100

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  # Author note: snapshotting turned off
  snapshot-after = off

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  # Author note: Set to one day - this may be a bit long for production.
  passivate-after-idle-timeout = 86400s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  # Author note: Made longer to support potentially slower Minikube environment
  ask-timeout = 60s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}

lagom.persistence.read-side {
  # how long should we wait when retrieving the last known offset
  # default is 5s, made longer for POC
  offset-timeout = 60s

  # Exponential backoff for failures in ReadSideProcessor
  failure-exponential-backoff {
    # minimum (initial) duration until processor is started again
    # after failure
    min = 3s

    # the exponential back-off is capped to this duration
    max = 30s

    # additional random delay is based on this factor
    random-factor = 0.2
  }

  # The amount of time that a node should wait for the global prepare callback to execute
  # default is 20s, made longer for POC
  global-prepare-timeout = 60s

  # Specifies that the read side processors should run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  run-on-role = ""

  # The Akka dispatcher to use for read-side actors and tasks.
  use-dispatcher = "lagom.persistence.dispatcher"
}

######################################
# Message Broker (Kafka) Configuration
######################################

lagom.broker.kafka {
  # The name of the Kafka service to look up out of the service locator.
  # If this is an empty string, then a service locator lookup will not be done,
  # and the brokers configuration will be used instead.
  service-name = "kafka_native"
  service-name = ${?KAFKA_SERVICE_NAME}

  # The URLs of the Kafka brokers. Separate each URL with a comma.
  # This will be ignored if the service-name configuration is non empty.
  brokers = ${lagom.broker.defaults.kafka.brokers}

  client {
    default {
      # Exponential backoff for failures
      failure-exponential-backoff {
        # minimum (initial) duration until processor is started again
        # after failure
        min = 3s

        # the exponential back-off is capped to this duration
        max = 30s

        # additional random delay is based on this factor
        random-factor = 0.2
      }
    }

    # configuration used by the Lagom Kafka producer
    producer = ${lagom.broker.kafka.client.default}
    producer.role = ""

    # configuration used by the Lagom Kafka consumer
    consumer {
      failure-exponential-backoff = ${lagom.broker.kafka.client.default.failure-exponential-backoff}

      # The number of offsets that will be buffered to allow the consumer flow to
      # do its own buffering. This should be set to a number that is at least as
      # large as the maximum amount of buffering that the consumer flow will do,
      # if the consumer buffer buffers more than this, the offset buffer will
      # backpressure and cause the stream to stop.
      offset-buffer = 100

      # Number of messages batched together by the consumer before the related messages'
      # offsets are committed to Kafka.
      # By increasing the batching-size you are trading speed with the risk of having
      # to re-process a larger number of messages if a failure occurs.
      # The value provided must be strictly greater than zero.
      batching-size = 20

      # Interval of time waited by the consumer before the currently batched messages'
      # offsets are committed to Kafka.
      # This parameter is useful to ensure that messages' offsets are always committed
      # within a fixed amount of time.
      # The value provided must be strictly greater than zero.
      batching-interval = 5 seconds
    }
  }
}
